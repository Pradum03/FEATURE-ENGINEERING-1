{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726d17fb-72bd-45f8-9cae-9a37482c67fe",
   "metadata": {},
   "source": [
    "ASSIGNMENT: FE-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af69d2-641b-4680-a16d-59b5417d1938",
   "metadata": {},
   "source": [
    "1. What are missing values in a dataset? Why is it essential to handle missing values? Name some \n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283face5-5e67-42a5-9855-6ffe9d785656",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data for one or more variables in a given observation. Missing values can occur for several reasons, including data entry errors, data corruption, or non-response by participants. The presence of missing values can lead to biased or inaccurate results in statistical analysis, modeling, and machine learning, which makes it crucial to handle them appropriately.\n",
    "\n",
    "It is essential to handle missing values because they can affect the accuracy and reliability of statistical models and machine learning algorithms. If missing values are not addressed, they can introduce bias, reduce the power of the analysis, and even lead to incorrect conclusions. Therefore, handling missing values is a critical step in the data preprocessing pipeline.\n",
    "\n",
    "There are several ways to handle missing values, such as deleting the rows or columns with missing values, imputing the missing values with the mean, median, mode, or other statistical methods, or using machine learning algorithms that can handle missing values.\n",
    "\n",
    "Some algorithms that are not affected by missing values include tree-based algorithms such as Random Forest and Gradient Boosting Machines, Bayesian networks, and deep learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). These algorithms can handle missing values in a robust and efficient manner, and they can provide accurate results even when the data contains missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1babd-ee5b-4ab9-a9dd-f39e6e30130f",
   "metadata": {},
   "source": [
    "2.  List down techniques used to handle missing data.  Give an example of each with python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87523db2-f4ae-4f82-9a9e-d97380190d37",
   "metadata": {},
   "source": [
    "Deletion method:\n",
    "This method involves removing the rows or columns with missing values. This approach is only suitable when the amount of missing data is small, and the missing data is missing at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab124af-ae6b-4320-a26c-790e215215fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B         C\n",
      "0  0.922022       NaN  0.923820\n",
      "1  0.253452  0.730530  0.336515\n",
      "2       NaN  0.049607  0.039550\n",
      "3  0.645468  0.275295       NaN\n",
      "4  0.879351  0.144878  0.693796\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate random data\n",
    "data = np.random.rand(5, 3)\n",
    "\n",
    "# Set some values to NaN\n",
    "data[0, 1] = np.nan\n",
    "data[2, 0] = np.nan\n",
    "data[3, 2] = np.nan\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'C'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f73e72f-5efa-49c2-be17-b31b81cb91f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    1\n",
       "B    1\n",
       "C    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545cef6b-6f06-4afd-b7b5-90aa7d082eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.253452</td>\n",
       "      <td>0.730530</td>\n",
       "      <td>0.336515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.879351</td>\n",
       "      <td>0.144878</td>\n",
       "      <td>0.693796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "1  0.253452  0.730530  0.336515\n",
       "4  0.879351  0.144878  0.693796"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()\n",
    "# deleting method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32552572-1d5c-47b6-913d-045beb35623b",
   "metadata": {},
   "source": [
    "Mean/Mode/Median imputation:\n",
    "\n",
    "This method involves filling the missing values with the mean, mode or median of the available data. This approach is suitable when the missing data is missing at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e84d13-9d92-40ca-8eed-f01bd4bfbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f737e42d-b81e-4a0e-90ac-7a7f77c57733",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean', add_indicator=False)   # putting mean at null\n",
    "df = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8694f31e-eb31-45e2-b086-c412ed5c579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, columns=['A', 'B', 'C'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "761e2c9d-c756-4516-9ca7-a907241d5093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922022</td>\n",
       "      <td>0.300077</td>\n",
       "      <td>0.923820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.253452</td>\n",
       "      <td>0.730530</td>\n",
       "      <td>0.336515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.675073</td>\n",
       "      <td>0.049607</td>\n",
       "      <td>0.039550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.645468</td>\n",
       "      <td>0.275295</td>\n",
       "      <td>0.498420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.879351</td>\n",
       "      <td>0.144878</td>\n",
       "      <td>0.693796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "0  0.922022  0.300077  0.923820\n",
       "1  0.253452  0.730530  0.336515\n",
       "2  0.675073  0.049607  0.039550\n",
       "3  0.645468  0.275295  0.498420\n",
       "4  0.879351  0.144878  0.693796"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f9c33f5-fce6-4ee1-98cf-af741bdf05bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2    3    4    5\n",
      "0  1.0   6.0  11.0  0.0  0.0  0.0\n",
      "1  2.0   8.5  12.0  0.0  1.0  0.0\n",
      "2  3.0   8.0  13.0  0.0  0.0  1.0\n",
      "3  2.5   9.0  14.0  1.0  0.0  0.0\n",
      "4  5.0  10.0  15.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, None, 5],\n",
    "    'B': [6, None, 8, 9, 10],\n",
    "    'C': [11, 12, None, 14, 15]\n",
    "})\n",
    "\n",
    "# Create an instance of SimpleImputer and fit_transform the data with add_indicator=True\n",
    "imputer = SimpleImputer(strategy='median', add_indicator=True)\n",
    "df = imputer.fit_transform(df)\n",
    "\n",
    "# Convert the NumPy array back to a Pandas DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e21249-de02-4bed-8e9c-78d8ebc9797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B    C\n",
      "0  1.0   6.0  cat\n",
      "1  2.0   7.0  dog\n",
      "2  3.0   8.0  cat\n",
      "3  4.0   7.5  dog\n",
      "4  5.0  10.0  cat\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1.0, 2.0, None, 4.0, 5.0],\n",
    "                   'B': [6.0, 7.0, 8.0, None, 10.0],\n",
    "                   'C': ['cat', 'dog', None, 'dog', 'cat']})\n",
    "\n",
    "# Fill missing values with mode\n",
    "df['C'] = df['C'].fillna(df['C'].mode()[0])\n",
    "df['A'] = df['A'].fillna(df['A'].mean())\n",
    "df['B'] = df['B'].fillna(df['B'].median())\n",
    "\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91237fca-b525-4f36-ad23-e64e3f6b3218",
   "metadata": {},
   "source": [
    "3. Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d719e57-dc45-4bdf-80a2-331e54fe5f10",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a dataset where the classes are not represented equally. For example, in a binary classification problem, if 90% of the data points belong to class A and only 10% belong to class B, the dataset is considered imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several problems:\n",
    "\n",
    "Biased model: In imbalanced data, the model may learn to predict the majority class, and not the minority class. This can result in a biased model, where the performance on the minority class is poor.\n",
    "Misclassification: The model may misclassify the minority class as the majority class, resulting in false negatives. This can be a critical problem in applications such as fraud detection, where missing a fraudulent transaction can lead to significant losses.\n",
    "Overfitting: In imbalanced data, the model may overfit to the majority class, resulting in poor generalization performance on new data.\n",
    "Therefore, it is essential to handle imbalanced data to build a fair and accurate model. There are several techniques to handle imbalanced data, such as:\n",
    "\n",
    "Resampling: Resampling techniques can be used to balance the classes. This can be achieved by either oversampling the minority class or undersampling the majority class.\n",
    "Cost-sensitive learning: Cost-sensitive learning involves assigning different misclassification costs to different classes. This can encourage the model to pay more attention to the minority class.\n",
    "Ensemble methods: Ensemble methods such as boosting and bagging can be used to combine multiple models and improve the performance on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b67714-6c41-4670-8c2e-db193db4e21c",
   "metadata": {},
   "source": [
    "4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down\u0002sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423c777-0dc0-4f71-bfb0-67984ca36238",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced data.\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class, whereas up-sampling involves increasing the number of instances in the minority class.\n",
    "\n",
    "For example, suppose we have a binary classification problem where the target variable has 100 positive instances and 10,000 negative instances. This is an imbalanced dataset because the number of negative instances far outweighs the number of positive instances.\n",
    "\n",
    "In this case, we can use down-sampling to reduce the number of negative instances to make the dataset more balanced. We could randomly select 100 negative instances and keep all 100 positive instances, resulting in a dataset with 200 instances.\n",
    "\n",
    "Alternatively, we could use up-sampling to increase the number of positive instances by generating new instances. We could use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate new instances that are similar to the existing positive instances. This would result in a dataset with a higher number of positive instances and a more balanced distribution between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cb33b-3cff-4135-97a3-d44f2e1c1a63",
   "metadata": {},
   "source": [
    "5. What is data Augmentation? Explain SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25add694-a9c0-4752-a7c0-577a433c3155",
   "metadata": {},
   "source": [
    "Data augmentation is the process of generating new training samples by augmenting the existing data using various techniques such as rotation, flipping, zooming, adding noise, and more. The goal is to increase the diversity and quantity of the training data to improve the model's performance.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique used to address class imbalance in classification tasks. It creates synthetic examples of the minority class by interpolating between minority class examples. The basic steps of SMOTE are:\n",
    "\n",
    "Identify the minority class examples that need oversampling.\n",
    "For each of the minority examples, identify k nearest neighbors from the same class.\n",
    "Randomly select one of the k-nearest neighbors and use it to create a synthetic example by interpolating between the selected neighbor and the original example.\n",
    "Repeat steps 2 and 3 until the desired number of new minority class examples are generated.\n",
    "For example, consider a binary classification problem where the positive class has only 10% of the total samples. This class imbalance can be addressed by applying SMOTE to the positive class to generate new synthetic examples. SMOTE can create synthetic samples that are slightly different from the original positive class samples but still belong to the positive class. This process can help balance the class distribution and improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191e37c-d464-43c1-ad78-99bb5de77caa",
   "metadata": {},
   "source": [
    "6. What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f9ea6-2dfc-4291-957c-2313217ccb76",
   "metadata": {},
   "source": [
    "Outliers are data points that deviate significantly from the other data points in a dataset. They are observations that lie an abnormal distance away from other values in a random sample from a population. Outliers can be either too high or too low with respect to the other values in the dataset. They can occur due to various reasons such as measurement errors, natural variations, or data entry errors.\n",
    "\n",
    "It is essential to handle outliers because they can significantly affect the results of data analysis. Outliers can affect the mean and standard deviation of the data, which can lead to biased statistical inferences. For instance, the mean of a dataset with outliers may not be a good representation of the central tendency of the data, leading to inaccurate conclusions. In machine learning, outliers can have a significant impact on the performance of the models. They can affect the accuracy of the predictions and lead to overfitting.\n",
    "\n",
    "Handling outliers can involve several techniques, such as removing them from the dataset, replacing them with the mean or median values, or transforming them using mathematical functions. The choice of the method depends on the nature of the data and the objectives of the analysis. It is important to carefully consider the impact of handling outliers on the data and to choose the most appropriate method for the specific context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a531e-17f1-42f3-91f3-be31a6fe5fcc",
   "metadata": {},
   "source": [
    "7. You are working on a project that requires analyzing customer data. However, you notice that some of \n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1748d8-b9d6-49d8-b6ea-502c8e8272d6",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in a dataset. Some of the commonly used techniques are:\n",
    "\n",
    "Deletion: This technique involves removing the rows or columns that contain missing data. This can be done using the dropna() function in pandas. However, this technique can result in a loss of information, and it should only be used when the amount of missing data is small.\n",
    "\n",
    "Imputation: This technique involves replacing missing values with estimated values. There are several imputation methods available, such as mean imputation, median imputation, mode imputation, and regression imputation. The choice of imputation method depends on the nature of the data and the amount of missing data.\n",
    "\n",
    "Using advanced algorithms: Advanced algorithms such as k-nearest neighbors (KNN) and decision trees can be used to impute missing values. These algorithms use the patterns in the data to estimate the missing values.\n",
    "\n",
    "Domain-specific knowledge: In some cases, domain-specific knowledge can be used to impute missing values. For example, if the missing data is related to a customer's income, their occupation, education level, and location can be used to estimate their income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc7855-84d5-4d25-b9ea-9b86a9c06e9e",
   "metadata": {},
   "source": [
    "8. You are working with a large dataset and find that a small percentage of the data is missing. What are \n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern \n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b31b1-b147-4ead-b76d-26a9a24e39cd",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are a few techniques:\n",
    "\n",
    "Missing Data Heatmap: A heatmap of missing values can provide insights into patterns of missing data. You can create a heatmap by visualizing the dataset with missing values, where missing values are colored in a different color.\n",
    "\n",
    "Correlation Matrix: A correlation matrix can be used to identify the relationship between the missing values and other variables in the dataset. By examining the correlation matrix, you may find that missing values are correlated with specific variables.\n",
    "\n",
    "Statistical Tests: Statistical tests such as Little's MCAR test, can be used to determine if missing data is missing completely at random or if there is a pattern to the missing data.\n",
    "\n",
    "Data Visualization: Data visualization techniques such as scatter plots, box plots, and histograms can be used to visualize the data and identify patterns or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0ee73-c837-4815-ba08-245505d1ec64",
   "metadata": {},
   "source": [
    "9.  Suppose you are working on a medical diagnosis project and find that the majority of patients in the \n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you \n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794e663-6c7c-4248-9625-dcf7e102b79f",
   "metadata": {},
   "source": [
    "Here are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "Confusion Matrix: A confusion matrix is a table that is often used to evaluate the performance of a classification model. It shows the actual class labels against the predicted class labels, and the number of correct and incorrect predictions. By analyzing the confusion matrix, we can calculate various performance metrics such as precision, recall, and F1 score, which are better suited for imbalanced datasets.\n",
    "\n",
    "Precision, Recall, and F1 Score: Precision measures the proportion of true positives (i.e., the number of correct positive predictions) among all positive predictions. Recall measures the proportion of true positives among all actual positive cases. F1 score is the harmonic mean of precision and recall, which takes both metrics into account. These metrics are useful for imbalanced datasets as they focus on the performance of the minority class.\n",
    "\n",
    "ROC Curve and AUC: ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier system. It plots the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds. AUC (Area Under the Curve) is a metric that represents the overall performance of the classifier. A higher AUC value indicates better performance.\n",
    "\n",
    "Stratified Sampling: In stratified sampling, we ensure that the proportions of the classes in the training and test sets are the same as those in the original dataset. This ensures that the model is trained on a representative sample of the data and can generalize well to new data.\n",
    "\n",
    "Resampling Techniques: Resampling techniques involve creating new samples by either oversampling the minority class or undersampling the majority class. Oversampling techniques such as SMOTE (Synthetic Minority Over-sampling Technique) can be used to create new synthetic samples for the minority class, while undersampling techniques such as random undersampling can be used to remove some of the majority class samples. These techniques can help balance the class distribution and improve the model's performance on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2c6f5-5a99-42fa-bc30-ad05b5698ece",
   "metadata": {},
   "source": [
    "10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is \n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to \n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a7821-c724-4efa-b297-b88e2a86a03c",
   "metadata": {},
   "source": [
    "In the scenario described, since the majority class represents satisfied customers, it may be important to carefully consider the trade-offs of down-sampling this group. One option could be to use a combination of down-sampling and SMOTE to balance the dataset and create new synthetic samples of the minority class. However, it is also important to evaluate the performance of the model on the imbalanced dataset and consider the potential impact of misclassification of the minority class. Additionally, it may be useful to explore other strategies to handle imbalanced datasets, such as cost-sensitive learning or using different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56514b-464d-4117-83e3-2a5636552173",
   "metadata": {},
   "source": [
    "To balance the unbalanced dataset and down-sample the majority class, the following methods can be employed:\n",
    "\n",
    "Random under-sampling: This involves randomly selecting a subset of the majority class to match the number of instances in the minority class. This can lead to a loss of information, and it may not be an effective method if the dataset is already small.\n",
    "\n",
    "Cluster-based under-sampling: This method involves clustering the majority class into groups and then undersampling from each cluster. This can help retain the information and the pattern present in the data.\n",
    "\n",
    "Synthetic minority over-sampling technique (SMOTE): This involves generating synthetic samples of the minority class by creating new instances based on existing minority instances. This helps to balance the dataset and ensure that the model is not overfitting on the minority class.\n",
    "\n",
    "Combined sampling: This involves combining different under-sampling and over-sampling techniques to achieve a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36af82c-5f80-45d4-bde8-245eb0faefdd",
   "metadata": {},
   "source": [
    "11. You discover that the dataset is unbalanced with a low percentage of occurrences while working on a \n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to \n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73323493-456f-4aa0-b2bb-0e5f2f0324f1",
   "metadata": {},
   "source": [
    "When dealing with a dataset that has a low percentage of occurrences of a rare event, you can use up-sampling techniques to balance the dataset. Some of the commonly used up-sampling techniques include:\n",
    "\n",
    "Random over-sampling: In this technique, the minority class samples are randomly duplicated until the number of samples in both the majority and minority classes is equal.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): In this technique, synthetic samples are generated for the minority class based on the existing samples, making it possible to create new samples that are representative of the minority class.\n",
    "\n",
    "ADASYN (Adaptive Synthetic Sampling): This technique generates synthetic samples for the minority class in regions of the feature space that are under-represented, making it possible to produce more samples that are representative of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f9f6e-cc4c-4018-8910-5549d835a717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
